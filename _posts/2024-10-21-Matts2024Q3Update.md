---
layout: post
nav-class: dark
categories: matt
title: CUDA comes to Math
author-id: matt
author-name: Matt Borland
---

Once again it has been a busy few months working on libraries in different stages of the boost lifecycle:

## Libraries for Proposal

### Decimal

Decimal (https://github.com/cppalliance/decimal) is a ground-up implementation of the IEEE 754 Decimal Floating Point types in C++14, and is co-authored by Chris Kormanyos.
The past few months we have worked hard to squeeze more performance out of the library, and we believe we have successfully accomplished that. 
By request we have also added functions to covert between the Binary Integer Significand Field (BID) and the Densely Packed Decimal Significand Field (DPD) encoding formats.
This allows you to not only interoperate with other software or hardware support that may exist now or in the future, but also to more store the results of the fast types on disk in a more compact fashion.
A few weeks ago we received endorsement for Boost formal review which is now scheduled for 13-22 January 2025.
Feel free to use the library and ask any questions prior to the review period.
Discussion of this library can be found on the Cpplang slack at `#boost-decimal`.

## Existing Libraries

### Math

As the headline says CUDA support has come to Boost.Math this summer! 
After much work and discussion with not only with the Boost maintainers but also with SciPy and CuPy maintainers we have added support for CUDA (NVCC and NVRTC), and SYCL to much of the library.
You will find many of the Special Functions, Statistical Distributions, and even some quadrature can now be run on device.
The design is to allow for seamless use whether the functions are being run on host or device.
All you need to do is add the functions into a kernel function and internal configuration will allow them to be run.
The only outlier is quadrature where the API is different to make things more efficient when running on device (pre-computed coefficient arrays are loaded into shared memory at compile time). 
Be sure to check the docs next release for the full listing of functionality that is now GPU enabled.
This winter we will continue to build out support in the library.
I'd like to thank the C++ Alliance for funding the CUDA CI infrastructure that made this development possible.

Looking forward to the winter Chris and I were looking for new places where existing libraries could be improved, and that would benefit from the massive parallelism that CUDA provides. 
We believe the domain of cryptography fits that bill.
Stay tuned.
